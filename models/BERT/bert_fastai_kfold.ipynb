{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "executionInfo": {
     "elapsed": 849,
     "status": "ok",
     "timestamp": 1602608520688,
     "user": {
      "displayName": "Vinícios Carvalho",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gjhp5co0d_AUKLWGcNdVqI7Qn0syOObU223e5VE7Hs=s64",
      "userId": "11287658652742546493"
     },
     "user_tz": 240
    },
    "id": "MEtoHJK4Owb8",
    "outputId": "5b180a81-4e45-4b98-aeeb-30d94dc8a166"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow 1.x selected.\n"
     ]
    }
   ],
   "source": [
    "%tensorflow_version 1.x\n",
    "SIZE = \"7k\"\n",
    "C = \"b\"\n",
    "CLASSES = 2\n",
    "FOLDER = \"\"\n",
    "FOLDS_PATH = \"\".format(SIZE,C)\n",
    "DATA_FOLDER = \"\"\n",
    "BATCH_SIZE = 48\n",
    "# LANG = \"BERT-MULTILINGUAL\"\n",
    "# model_name = \"bert-base-multilingual-cased\"\n",
    "# LANG = \"BERT-PORTUGUESE\"\n",
    "# model_name = \"neuralmind/bert-base-portuguese-cased\"\n",
    "\n",
    "# BATCH_SIZE = 4\n",
    "# LANG = \"BERT-LARGE-MULTILINGUAL\"\n",
    "# model_name = \"bert-large-cased\"\n",
    "# LANG = \"BERT-LARGE-PORTUGUESE\"\n",
    "# model_name = \"neuralmind/bert-large-portuguese-cased\"\n",
    "CLASS_NAME = {2:['Sem Risco', 'Risco Potencial'], 3:['Sem Risco', 'Risco Potencial', 'Risco Alto']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 340
    },
    "executionInfo": {
     "elapsed": 4569,
     "status": "ok",
     "timestamp": 1602608524417,
     "user": {
      "displayName": "Vinícios Carvalho",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gjhp5co0d_AUKLWGcNdVqI7Qn0syOObU223e5VE7Hs=s64",
      "userId": "11287658652742546493"
     },
     "user_tz": 240
    },
    "id": "AUkwQtGOuIp1",
    "outputId": "dbce6c9b-fbd2-4e78-ba32-4d78ae824fde"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /usr/local/lib/python3.6/dist-packages (3.3.1)\n",
      "Requirement already satisfied: tokenizers==0.8.1.rc2 in /usr/local/lib/python3.6/dist-packages (from transformers) (0.8.1rc2)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.23.0)\n",
      "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers) (0.7)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from transformers) (20.4)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n",
      "Requirement already satisfied: sentencepiece!=0.1.92 in /usr/local/lib/python3.6/dist-packages (from transformers) (0.1.91)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.18.5)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.41.1)\n",
      "Requirement already satisfied: sacremoses in /usr/local/lib/python3.6/dist-packages (from transformers) (0.0.43)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2020.6.20)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.10)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->transformers) (2.4.7)\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from packaging->transformers) (1.15.0)\n",
      "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (0.16.0)\n",
      "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.1.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GJOxdU8Ixqq7"
   },
   "source": [
    "# Carregando o Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "executionInfo": {
     "elapsed": 4562,
     "status": "ok",
     "timestamp": 1602608524418,
     "user": {
      "displayName": "Vinícios Carvalho",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gjhp5co0d_AUKLWGcNdVqI7Qn0syOObU223e5VE7Hs=s64",
      "userId": "11287658652742546493"
     },
     "user_tz": 240
    },
    "id": "8nhAhfDExp6p",
    "outputId": "5a80cca3-c574-4881-8c8d-4df5179ae49e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "executionInfo": {
     "elapsed": 4934,
     "status": "ok",
     "timestamp": 1602608524800,
     "user": {
      "displayName": "Vinícios Carvalho",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gjhp5co0d_AUKLWGcNdVqI7Qn0syOObU223e5VE7Hs=s64",
      "userId": "11287658652742546493"
     },
     "user_tz": 240
    },
    "id": "4FyFUZKwx2DM"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from time import ctime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Tl5yCQrwziVQ"
   },
   "source": [
    "# HuggingFace Transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "executionInfo": {
     "elapsed": 7126,
     "status": "ok",
     "timestamp": 1602608526999,
     "user": {
      "displayName": "Vinícios Carvalho",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gjhp5co0d_AUKLWGcNdVqI7Qn0syOObU223e5VE7Hs=s64",
      "userId": "11287658652742546493"
     },
     "user_tz": 240
    },
    "id": "b2uFIotszatf"
   },
   "outputs": [],
   "source": [
    "from transformers import BertForSequenceClassification, BertTokenizer, BertConfig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "03zbi81g0CDB"
   },
   "source": [
    "## Integração com FastAi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "executionInfo": {
     "elapsed": 7612,
     "status": "ok",
     "timestamp": 1602608527491,
     "user": {
      "displayName": "Vinícios Carvalho",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gjhp5co0d_AUKLWGcNdVqI7Qn0syOObU223e5VE7Hs=s64",
      "userId": "11287658652742546493"
     },
     "user_tz": 240
    },
    "id": "2uMlboM50Wg4"
   },
   "outputs": [],
   "source": [
    "from fastai import *\n",
    "from fastai.text import *\n",
    "from fastai.callbacks import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DYBJOT7D0mXR"
   },
   "source": [
    "**Tokenizador:** classe responsável por tokenizar o texto, sendo criada a partir da classe BaseTokenizer do FastAi. Além de tokenizar o texto utilizando um tokenizador já treinado, o mesmo adiciona os tokens especiais CLS e SEP necessários para o BERT.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "executionInfo": {
     "elapsed": 8828,
     "status": "ok",
     "timestamp": 1602608528714,
     "user": {
      "displayName": "Vinícios Carvalho",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gjhp5co0d_AUKLWGcNdVqI7Qn0syOObU223e5VE7Hs=s64",
      "userId": "11287658652742546493"
     },
     "user_tz": 240
    },
    "id": "14l43wdXzm-G"
   },
   "outputs": [],
   "source": [
    "class TransformersBaseTokenizer(BaseTokenizer):\n",
    "    \"\"\"Wrapper around PreTrainedTokenizer to be compatible with fast.ai\"\"\"\n",
    "    def __init__(self, pretrained_tokenizer, seq_len=512, model_type = 'bert', **kwargs):\n",
    "        self._pretrained_tokenizer = pretrained_tokenizer\n",
    "        self.max_seq_len = seq_len\n",
    "        self.model_type = model_type\n",
    "\n",
    "    def __call__(self, *args, **kwargs): \n",
    "        return self\n",
    "\n",
    "    def tokenizer(self, t:str) -> List[str]:\n",
    "        \"\"\"Limits the maximum sequence length and add the spesial tokens\"\"\"\n",
    "        CLS = self._pretrained_tokenizer.cls_token\n",
    "        SEP = self._pretrained_tokenizer.sep_token\n",
    "        if self.model_type in ['roberta']:\n",
    "            tokens = self._pretrained_tokenizer.tokenize(t, add_prefix_space=True)[:self.max_seq_len - 2]\n",
    "        else:\n",
    "            tokens = self._pretrained_tokenizer.tokenize(t)[:self.max_seq_len - 2]\n",
    "        return [CLS] + tokens + [SEP]\n",
    "        \n",
    "transformer_tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "transformer_base_tokenizer = TransformersBaseTokenizer(pretrained_tokenizer = transformer_tokenizer, model_type = model_type)\n",
    "fastai_tokenizer = Tokenizer(tok_func = transformer_base_tokenizer, pre_rules=[], post_rules=[])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JF7Qmymq1Z21"
   },
   "source": [
    "**Numericalizer:** classe responsável por converter tokens em índices(inteiros) que serão utilizados como entrada para o BERT. A conversão em índices é feita baseando-se em um vocabulário pré-definido."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "executionInfo": {
     "elapsed": 8819,
     "status": "ok",
     "timestamp": 1602608528715,
     "user": {
      "displayName": "Vinícios Carvalho",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gjhp5co0d_AUKLWGcNdVqI7Qn0syOObU223e5VE7Hs=s64",
      "userId": "11287658652742546493"
     },
     "user_tz": 240
    },
    "id": "fHvZFOU11ImS"
   },
   "outputs": [],
   "source": [
    "class TransformersVocab(Vocab):\n",
    "    def __init__(self, tokenizer):\n",
    "        super(TransformersVocab, self).__init__(itos = [])\n",
    "        self.tokenizer = tokenizer\n",
    "    \n",
    "    def numericalize(self, t:Collection[str]) -> List[int]:\n",
    "        \"Convert a list of tokens `t` to their ids.\"\n",
    "        return self.tokenizer.convert_tokens_to_ids(t)\n",
    "        #return self.tokenizer.encode(t)\n",
    "\n",
    "    def textify(self, nums:Collection[int], sep=' ') -> List[str]:\n",
    "        \"Convert a list of `nums` to their tokens.\"\n",
    "        nums = np.array(nums).tolist()\n",
    "        return sep.join(self.tokenizer.convert_ids_to_tokens(nums)) if sep is not None else self.tokenizer.convert_ids_to_tokens(nums)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3hURp2Yn12pZ"
   },
   "source": [
    "# FastAi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "executionInfo": {
     "elapsed": 8813,
     "status": "ok",
     "timestamp": 1602608528716,
     "user": {
      "displayName": "Vinícios Carvalho",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gjhp5co0d_AUKLWGcNdVqI7Qn0syOObU223e5VE7Hs=s64",
      "userId": "11287658652742546493"
     },
     "user_tz": 240
    },
    "id": "7na5nynm3i17"
   },
   "outputs": [],
   "source": [
    "bs = BATCH_SIZE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "executionInfo": {
     "elapsed": 8807,
     "status": "ok",
     "timestamp": 1602608528717,
     "user": {
      "displayName": "Vinícios Carvalho",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gjhp5co0d_AUKLWGcNdVqI7Qn0syOObU223e5VE7Hs=s64",
      "userId": "11287658652742546493"
     },
     "user_tz": 240
    },
    "id": "xMM0HmZU1UFh"
   },
   "outputs": [],
   "source": [
    "transformer_vocab =  TransformersVocab(tokenizer=transformer_tokenizer)\n",
    "numericalize_processor = NumericalizeProcessor(vocab=transformer_vocab)\n",
    "# False para prevenir a adição de tokens desnecessários pelo processador\n",
    "tokenize_processor = TokenizeProcessor(tokenizer=fastai_tokenizer, \n",
    "                                       include_bos=False, \n",
    "                                       include_eos=False)\n",
    "\n",
    "transformer_processor = [tokenize_processor, numericalize_processor]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "executionInfo": {
     "elapsed": 8800,
     "status": "ok",
     "timestamp": 1602608528718,
     "user": {
      "displayName": "Vinícios Carvalho",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gjhp5co0d_AUKLWGcNdVqI7Qn0syOObU223e5VE7Hs=s64",
      "userId": "11287658652742546493"
     },
     "user_tz": 240
    },
    "id": "aitf5LCt164Q"
   },
   "outputs": [],
   "source": [
    "pad_first = bool(model_type in ['xlnet'])\n",
    "pad_idx = transformer_tokenizer.pad_token_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ifOlOXVy3qha"
   },
   "source": [
    "# Encapsulando o BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "executionInfo": {
     "elapsed": 8792,
     "status": "ok",
     "timestamp": 1602608528718,
     "user": {
      "displayName": "Vinícios Carvalho",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gjhp5co0d_AUKLWGcNdVqI7Qn0syOObU223e5VE7Hs=s64",
      "userId": "11287658652742546493"
     },
     "user_tz": 240
    },
    "id": "JtM2Uf2V3EON"
   },
   "outputs": [],
   "source": [
    "class BERT(nn.Module):\n",
    "  \n",
    "    def __init__(self, transformer_model):\n",
    "        super(BERT,self).__init__()\n",
    "        self.transformer = transformer_model\n",
    "        \n",
    "    def forward(self, input_ids):\n",
    "        # Return only the logits from the transfomer\n",
    "        logits = self.transformer(input_ids)[0]   \n",
    "        return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CgTqg1g_Z2i0"
   },
   "source": [
    "#Carregando os folds e realizando o treino em cada um"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 518
    },
    "id": "BeG_DhqB4EGS",
    "outputId": "148fdee9-d90d-4243-ebaf-14dd9f0583e9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 0 \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-large-cased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-large-cased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f_beta</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.691455</td>\n",
       "      <td>0.665068</td>\n",
       "      <td>0.618297</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.382066</td>\n",
       "      <td>11:39</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/fastai/text/data.py:339: UserWarning: This overload of nonzero is deprecated:\n",
      "\tnonzero()\n",
      "Consider using one of the following signatures instead:\n",
      "\tnonzero(*, bool as_tuple) (Triggered internally at  /pytorch/torch/csrc/utils/python_arg_parser.cpp:766.)\n",
      "  idx_min = (t != self.pad_idx).nonzero().min()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this Learner object self-destroyed - it still exists, but no longer usable\n",
      "Fold 1 \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-large-cased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-large-cased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "                background: #F44336;\n",
       "            }\n",
       "        </style>\n",
       "      <progress value='0' class='' max='1' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      0.00% [0/1 00:00<00:00]\n",
       "    </div>\n",
       "    \n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f_beta</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>\n",
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "                background: #F44336;\n",
       "            }\n",
       "        </style>\n",
       "      <progress value='162' class='' max='1427' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      11.35% [162/1427 01:21<10:39 0.6817]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AdamW\n",
    "\n",
    "folds = int(len([name for name in os.listdir(FOLDS_PATH) if os.path.isfile(os.path.join(FOLDS_PATH,name))]) / 2)\n",
    "\n",
    "val_acc = []\n",
    "val_prec = []\n",
    "val_rec = []\n",
    "val_f1 = []\n",
    "conf_matrix = np.zeros(CLASSES*CLASSES).reshape(CLASSES, -1)\n",
    "\n",
    "for f in range(folds):\n",
    "  \n",
    "  print(\"Fold\",f,'\\n')\n",
    "  train_df = pd.read_csv(FOLDS_PATH + \"train\"+str(f) + \".csv\", sep=\"\\t\", index_col=False)\n",
    "  test_df = pd.read_csv(FOLDS_PATH + \"test\"+str(f) + \".csv\", sep=\"\\t\", index_col=False)\n",
    "  y_true = test_df['y'].tolist()\n",
    "  texts = test_df['text'].tolist()\n",
    "  CLASSES = len(test_df['y'].unique())\n",
    "\n",
    "  transformer_model = BertForSequenceClassification.from_pretrained(model_name, num_labels=CLASSES)\n",
    "  transformer_model = BERT(transformer_model=transformer_model)\n",
    "  transformer_model = transformer_model.cuda()\n",
    "\n",
    "  \n",
    "  databunch = (TextList.from_df(train_df, cols='text', processor=transformer_processor)\n",
    "             .split_by_rand_pct(0.1,seed=9999)\n",
    "             .label_from_df(cols= 'y')\n",
    "             .add_test(test_df)\n",
    "             .databunch(bs=bs, pad_first=pad_first, pad_idx=pad_idx))\n",
    "  \n",
    "  learner = Learner(databunch, \n",
    "                  transformer_model, \n",
    "                  opt_func = lambda input: AdamW(input, correct_bias=False), \n",
    "                  metrics=[accuracy, Precision(average=\"macro\"), Recall(average=\"macro\"), FBeta(average=\"macro\", beta=1)])\n",
    "  # learner.lr_find()\n",
    "  # learner.recorder.plot()\n",
    "  learner.fit_one_cycle(1, max_lr=1e-5)\n",
    "  # learner.fit_one_cycle(2, max_lr=1e-6)\n",
    "  # learner.fit_one_cycle(3, max_lr=1e-6)\n",
    "\n",
    "  loss_value, acc_value, prec_value, rec_value, f1_value = learner.validate()\n",
    "  val_acc.append(acc_value.item())\n",
    "  val_prec.append(prec_value.item())\n",
    "  val_rec.append(rec_value.item())\n",
    "  val_f1.append(f1_value.item())\n",
    "\n",
    "  preds = [learner.predict(text) for text in texts]\n",
    "  p = np.array(preds)\n",
    "  y_pred = [int(x) for x in p[:,0]]\n",
    "  \n",
    "  conf_matrix += confusion_matrix(y_true, y_pred)\n",
    "  \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "  learner.destroy()\n",
    "print(conf_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "a72KdWKS5CU5"
   },
   "outputs": [],
   "source": [
    "saida = {'acc': val_acc, 'prec': val_prec, 'rec': val_rec, 'f1':val_f1}\n",
    "df = pd.DataFrame(saida)\n",
    "res = {'acc_avg': np.mean(val_acc), 'prec_avg': np.mean(val_prec), 'rec_avg':np.mean(val_rec), 'f1_avg':np.mean(val_f1), 'acc_std': np.std(val_acc), 'prec_std': np.std(val_prec), 'rec_std':np.std(val_rec), 'f1_std':np.std(val_f1)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "q6T560QDfYTt"
   },
   "outputs": [],
   "source": [
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "HVC1kUpWfENa"
   },
   "outputs": [],
   "source": [
    "created_at = ctime()\n",
    "created_at = created_at.replace(' ', '-')\n",
    "mean_std = pd.DataFrame(res, index=[0])\n",
    "mean_std.head()\n",
    "name = \"{}-{}-{}-{}-class-results.csv\".format(LANG, SIZE, created_at, CLASSES)\n",
    "path = os.path.join(FOLDER, 'results', name)\n",
    "mean_std.to_csv(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "m-IPgop7GHXH"
   },
   "outputs": [],
   "source": [
    " p = np.array(preds)\n",
    "y_pred = [int(x) for x in p[:,0]]\n",
    "y_true = test_df['y'].tolist()\n",
    "matrix = confusion_matrix(y_true, y_pred)\n",
    "print(matrix)\n",
    "print(sum(sum(matrix)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j2x71g2aVqd6"
   },
   "source": [
    "#Salvando resultados no arquivo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "OamriPhHz0nO"
   },
   "outputs": [],
   "source": [
    "# LANG = \"BERT-MULTILINGUAL\"\n",
    "created_at = ctime()\n",
    "created_at = created_at.replace(' ', '-')\n",
    "name = \"{}-{}-{}-{}-class-results.csv\".format(LANG, SIZE, created_at, CLASSES)\n",
    "with open(os.path.join(FOLDER, 'results', name), 'w') as f:\n",
    "  f.write(\"folds, acc, prec, rec, f1\\n\")\n",
    "  f.write(\"{},{},{},{},{}\\n\".format(folds, np.mean(val_acc), np.mean(val_prec), np.mean(val_rec), np.mean(val_f1)))  \n",
    "  f.write(\"{},{},{},{},{}\\n\".format('-', acc, prec, rec, f1))  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P8N7WLsRM9uu"
   },
   "source": [
    "## Salvando modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "7lGwvaJALwnZ"
   },
   "outputs": [],
   "source": [
    "# name = \"{}-{}-{}-{}-class-model\".format(LANG, SIZE, created_at, CLASSES)\n",
    "# learner.save(FOLDER+name, return_path=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "LLJwYKbqNB7w"
   },
   "outputs": [],
   "source": [
    "# learner.recorder.metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JzGVvwJnwJlN"
   },
   "source": [
    "# Gerando predições"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FiF1Q6CUzjny"
   },
   "source": [
    "1.   0: Sem Risco;\n",
    "2.   1: Risco em Potencial;\n",
    "3.   2: Risco alto;\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "hKCRC1OAwI94"
   },
   "outputs": [],
   "source": [
    "def get_preds_as_nparray(ds_type) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    the get_preds method does not yield the elements in order by default\n",
    "    we borrow the code from the RNNLearner to resort the elements into their correct order\n",
    "    \"\"\"\n",
    "    preds = learner.get_preds(ds_type)[0].detach().cpu().numpy()\n",
    "    sampler = [i for i in databunch.dl(ds_type).sampler]\n",
    "    reverse_sampler = np.argsort(sampler)\n",
    "    return preds[reverse_sampler, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "qPCclTyrwQwk"
   },
   "outputs": [],
   "source": [
    "test_preds = get_preds_as_nparray(DatasetType.Test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "91x5RqSkxx24"
   },
   "outputs": [],
   "source": [
    "test_df[\"prediction\"] = np.argmax(test_preds,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "a5fSPB_R11xr"
   },
   "outputs": [],
   "source": [
    "print(test_df['y'].iloc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "GeIuoo0Ex1Ai"
   },
   "outputs": [],
   "source": [
    "for i in range(100):\n",
    "  print(test_df['text'].iloc[i])\n",
    "  print(\"label: {}, prediction: {}\".format(test_df['y'].iloc[i], test_df['prediction'].iloc[i], '\\n\\n'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "UOUmER4yyMQ7"
   },
   "outputs": [],
   "source": [
    "for i in range(CLASSES):\n",
    "  suicide_true = test_df.loc[test_df['y'] == i]['y'].count()\n",
    "  suicide_pred = test_df.loc[test_df['y'] == i].loc[test_df['prediction']==i]['y'].count()\n",
    "  print(\"Accuracy on class\", CLASS_NAME[CLASSES][i], ':', suicide_pred/suicide_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "O6s1CCzozRmT"
   },
   "outputs": [],
   "source": [
    "test_df['y'].size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "mben4o8ITs4t"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "bert_fastai_kfold.ipynb",
   "provenance": [
    {
     "file_id": "11E-J09W51D2fkytK7ZBwJ-CIkNo7a_6L",
     "timestamp": 1585247214114
    }
   ],
   "version": ""
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
